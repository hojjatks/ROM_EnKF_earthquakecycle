{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This is a copy of MLmodel8postprocess.\n",
    "This code is written to use and integrate the models that were learned in `MLmodel8.ipynb` and `MLmodel8_timestep.ipynb` and also plot some of the scaling laws that we are interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do\n",
    "We need to seperate the snapshot learning and time step learning first and then try to integrate them together to plot the scaling laws.\n",
    "## Snapshot Learning:\n",
    "* load the data (done)\n",
    "* load the model (done)\n",
    "* plot the invariant measure (done)\n",
    "* implement the code for N_m!=30 (done)\n",
    "* timestep (done for N_m=30)\n",
    "* timestep (for N_m!30) (done)\n",
    "* plot the scaling laws\n",
    "* saving figures\n",
    "* calculate the time it takes to run the simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapshot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ProcessFunctions import find_Aisv2,Find_a_i,find_Aisv2_onlyv,Find_T_X_tau_without_p_input\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns \n",
    "from scipy import integrate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Area=320e3*50e3\n",
    "mu=30e9\n",
    "V_thresh=(5e-8)\n",
    "findscaling=1\n",
    "t_yr=60*60*24*365\n",
    "Nx=256\n",
    "Nz=32\n",
    "L_thresh=1e3\n",
    "L=320e3\n",
    "L_fault=L\n",
    "W=50e3\n",
    "x_ox=np.linspace(-L/2,L/2,Nx)\n",
    "z_ox=np.linspace(0,W,Nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMwupdated(V_full,t_full,V_thresh):\n",
    "    flag=0\n",
    "    V_max=np.max(V_full,axis=(1,2))\n",
    "    Mw=np.array([])\n",
    "    T1=np.array([]) # it is the time of when the earthquakes nucleate\n",
    "    T2=np.array([]) # it is the time of when the earthquake stops\n",
    "    PotRate=np.sum(V_full,axis=(1,2))*Area/Nx/Nz\n",
    "\n",
    "    for i in range(V_max.size):\n",
    "        \n",
    "        if flag==0 and V_max[i]>V_thresh: # an event has started\n",
    "            flag=1\n",
    "            index1=i\n",
    "            T1=np.append(T1,t_full[i])\n",
    "        if flag==1 and V_max[i]<V_thresh: # the event has stopped\n",
    "            flag=0\n",
    "            index2=i\n",
    "            IntPotRate=integrate.cumtrapz(PotRate[index1:index2+1],t_full[index1:index2+1])\n",
    "            Integration=IntPotRate[-1]\n",
    "            M0=Integration*mu\n",
    "            Mw=np.append(Mw,(2/3)*np.log10(M0)-6)\n",
    "            T2=np.append(T2,t_full[i])\n",
    "    return Mw,T1,T2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading POD modes\n",
    "data_dir=\"/central/groups/astuart/hkaveh/Data/LearnROM/MainSimulation_Tf600Nt=5PODonlyonV.npz\"\n",
    "PODmodes=np.load(data_dir)\n",
    "U_v=PODmodes['U']\n",
    "S_v=PODmodes['S']\n",
    "VT_v=PODmodes['VT']\n",
    "q_bar_v=PODmodes['q_bar']\n",
    "\n",
    "data_dir=\"/central/groups/astuart/hkaveh/Data/LearnROM/MainSimulation_Tf600Nt=5PODonlyontheta.npz\"\n",
    "PODmodes=np.load(data_dir)\n",
    "U_theta=PODmodes['U']\n",
    "S_theta=PODmodes['S']\n",
    "VT_theta=PODmodes['VT']\n",
    "q_bar_theta=PODmodes['q_bar']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the simulation\n",
    "# Loading data from \"/central/groups/astuart/hkaveh/Data/LearnROM/\"\n",
    "Tf = 250 # each initial condition is simulated for 250 years\n",
    "Nt=5     # it is recorded every Nt time stes\n",
    "N_m_v=20 # number of modes that they have considered\n",
    "N_m_theta=20 # number of modes that they have considered\n",
    "N_m_load=30\n",
    "coeff=2 # How the inital conditions are from the chaotic attractor\n",
    "N_cut=2000\n",
    "filter_ratio=0.4 # removing 40 percent of the data\n",
    "# loading time series:\n",
    "X_full=np.empty((1,N_m_load*2))\n",
    "\n",
    "phi=U_v[:,:N_m_load]    # This contains the eigen mode for velocity\n",
    "V_max_QDYN=np.empty((1,1))\n",
    "Y_full=np.empty((1,1))\n",
    "Mws=np.array([])\n",
    "T1s=np.array([])\n",
    "T2s=np.array([])\n",
    "Mws_v2=np.array([])\n",
    "T1s_v2=np.array([])\n",
    "T2s_v2=np.array([])\n",
    "rectangles_all=np.array([])\n",
    "# The pick in the tail of the pdf is for one of the samples in the range 55 to 60\n",
    "for number in range(100):\n",
    "    data_dir=\"/central/groups/astuart/hkaveh/Data/LearnROM/Dataset2/SampleSimulation_Tf\"+str(Tf)+\"Nt=\"+str(Nt)+\"N_m\"+str(N_m_load)+\"coeff\"+str(coeff)+\"number\"+str(number)+\".npz\"\n",
    "    data_smaple=np.load(data_dir)\n",
    "    V_ox=data_smaple['array1']\n",
    "    theta_ox=data_smaple['array2']\n",
    "    t_ox=data_smaple['array3']\n",
    "    # We work we log10 of V_ox:\n",
    "    t=t_ox[:,0,0].reshape(-1,1)\n",
    "    Start_index=int(V_ox.shape[0]*filter_ratio)\n",
    "    if findscaling==1:\n",
    "        Mw,T1,T2=FindMwupdated(V_ox[Start_index:,:,:],t_ox[Start_index:,0,0],V_thresh)\n",
    "        Mws=np.append(Mws,Mw)\n",
    "        T1s=np.append(T1s,T1)\n",
    "        T2s=np.append(T2s,T2)\n",
    "        TimeStarts,TimeEnds,rectangles,Mags=Find_T_X_tau_without_p_input(V_ox,t_ox,V_thresh,L_thresh,t_yr,x_ox,z_ox,L_fault,mu)\n",
    "\n",
    "        Mws_v2=np.append(Mws_v2,Mags)\n",
    "        T1s_v2=np.append(T1s_v2,TimeStarts)\n",
    "        T2s_v2=np.append(T2s_v2,TimeEnds)\n",
    "        rectangles_all=np.append(rectangles_all,rectangles)\n",
    "    dt=t[1:]-t[:-1]\n",
    "    dt=np.log10(dt)\n",
    "\n",
    "    V_ox=np.log10(V_ox)\n",
    "    theta_ox=np.log10(theta_ox)\n",
    "\n",
    "    V_max_onesimulation=np.max(V_ox,axis=(1,2))\n",
    "\n",
    "    A_v,P_v=find_Aisv2_onlyv(U_v,V_ox,q_bar_v,N_m_load) # you dont need P in general, but I need it to check if I do everything correctly\n",
    "    A_theta,P_theta=find_Aisv2_onlyv(U_theta,theta_ox,q_bar_theta,N_m_load)\n",
    "    A=np.concatenate((A_v,A_theta),axis=1)\n",
    "    \n",
    "    X_onesimulation=A[Start_index:-1,:] # with removig the first 20 percent of the data to remove the transient\n",
    "    Y_onesimulation=dt[Start_index:]\n",
    "    V_max_onesimulation=V_max_onesimulation[Start_index:]\n",
    "    X_full=np.append(X_full,X_onesimulation,axis=0)\n",
    "    Y_full=np.append(Y_full,Y_onesimulation,axis=0)\n",
    "\n",
    "    print(\"number is \" +str(number)+ \" size of data in this simulation is \" +str(X_onesimulation.shape[0])+\" total size is \" + str(X_full.shape[0]))\n",
    "    V_max_QDYN=np.append(V_max_QDYN,V_max_onesimulation[:-1])\n",
    "\n",
    "\n",
    "V_max_QDYN=V_max_QDYN[1:]\n",
    "X_full=X_full[1:,:]\n",
    "Y_full=Y_full[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T2s.shape)\n",
    "print(T1s.shape)\n",
    "print(Mws.shape)\n",
    "print(T2s_v2.shape)\n",
    "print(T1s_v2.shape)\n",
    "print(Mws_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis objects\n",
    "fig, axs = plt.subplots(3, 5, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axs array so that we can iterate over it easily\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot the KDE plot for each column of X_full\n",
    "for i in range(15):\n",
    "    sns.kdeplot(X_full[:,i], ax=axs[i])\n",
    "    axs[i].set_title(r'$\\alpha_{{{}}}$'.format(i+1))\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Defining model\n",
    "class Forwardmap(nn.Module):\n",
    "    def __init__(self,N_m):\n",
    "        super().__init__()\n",
    "        self.hidden1=nn.Linear(N_m,2*N_m)\n",
    "        self.act1=nn.Tanh()\n",
    "        self.hidden2=nn.Linear(2*N_m,4*N_m)\n",
    "        self.act2=nn.Tanh()\n",
    "        self.hidden3=nn.Linear(4*N_m,4*N_m)\n",
    "        self.act3=nn.Tanh()\n",
    "        self.hidden4=nn.Linear(4*N_m,2*N_m)\n",
    "        self.act4=nn.Tanh()\n",
    "        self.output=nn.Linear(2*N_m,N_m)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.act1(self.hidden1(x))\n",
    "        x=self.act2(self.hidden2(x))\n",
    "        x=self.act3(self.hidden3(x))\n",
    "        x=self.act4(self.hidden4(x))\n",
    "        x=(self.output(x))\n",
    "        return x\n",
    "    \n",
    "version=2024.11\n",
    "model=Forwardmap(N_m_v+N_m_theta)\n",
    "model.load_state_dict(torch.load( \"/central/groups/astuart/hkaveh/Data/LearnROM/ROM_POD_separate_onV_theta\"+str(version)+\"N_m_v\"+str(N_m_v)+\"N_m_theta\"+str(N_m_theta)+\".pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the ML model for a long time\n",
    "# remember your input is scaled and the output is scaled too.\n",
    "# For now, I am use the output file but later you should save the max_X and max_y\n",
    "# max_X=928.8324574287669\n",
    "# max_y=26.351065203181317 # for version 0\n",
    "# max_X=928.8324574287669\n",
    "# max_y=13.638293616464068\n",
    "\n",
    "\n",
    "\n",
    "max_X=657.090180396257\n",
    "max_y=28.19128123215918\n",
    "\n",
    "xk_v_0=X_full[-1,:N_m_v]\n",
    "xk_theta_0=X_full[-1,N_m_load:N_m_load+N_m_theta]\n",
    "xk=np.append(xk_v_0,xk_theta_0).reshape(1,N_m_v+N_m_theta)/max_X # We use the scaled coordinates\n",
    "\n",
    "# xk=X_full[-1,:].reshape(1,N_m_v+N_m_theta)/max_X # We use the scaled coordinates\n",
    "xk=torch.tensor(xk, dtype=torch.float32)\n",
    "xk.to(device)\n",
    "\n",
    "forecast30=xk.detach().numpy().reshape(N_m_v+N_m_theta,1)\n",
    "# set the device for forecast30 to device\n",
    "# forecast30.to(device)\n",
    "\n",
    "\n",
    "for index in range(10*N_cut):\n",
    "    ykp1=(model(xk))*(max_y/max_X)\n",
    "    xkp1=ykp1+xk\n",
    "    numpy_vector = xkp1.detach().numpy().reshape(N_m_v+N_m_theta,1)\n",
    "    forecast30 = np.hstack([forecast30, numpy_vector])\n",
    "    xk=xkp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecast30[0,:])\n",
    "plt.plot(forecast30[1,:])\n",
    "plt.plot(forecast30[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the invariant measure in the pytorch model\n",
    "\n",
    "fig, axs = plt.subplots(3, 5, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axs array so that we can iterate over it easily\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot the KDE plot for each column of X_full\n",
    "for i in range(15):\n",
    "    sns.kdeplot(forecast30[i,:]*max_X, ax=axs[i],label='ROM n='+str(N_m_theta+N_m_v))\n",
    "    sns.kdeplot(X_full[:, i], ax=axs[i],label='QDYN')\n",
    "    axs[i].set_title(r'$\\alpha_{{{}}}$'.format(i+1))\n",
    "    axs[i].legend()\n",
    "\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the code for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_Nv=np.array([20,25,30])\n",
    "\n",
    "list_Ntheta=np.array([20,25,30])\n",
    "# max_X_list=np.array([928.8324574287669, 928.8324574287669,928.8324574287669])    # For version 0\n",
    "max_X_list=np.array([657.090180396257, 657.090180396257,698.3903391568679])    # For version 2024.11\n",
    "#max_y_list=np.array([26.351065203181317,26.351065203181317,26.351065203181317]) # For version 0\n",
    "#max_y_list=np.array([13.638293616464068,13.638293616464068,13.638293616464068])  # For version 96\n",
    "max_y_list=np.array([28.19128123215918,28.19128123215918,25.87581902121011])  # For version 2024.11\n",
    "\n",
    "\n",
    "version=2024.11\n",
    "N_it=25000\n",
    "#N_it=1000\n",
    "forecasts=[]\n",
    "X_MLs=[]\n",
    "for i in range(list_Nv.size):\n",
    "    # loading the model\n",
    "    model=Forwardmap(list_Nv[i]+list_Ntheta[i])\n",
    "    model.load_state_dict(torch.load( \"/central/groups/astuart/hkaveh/Data/LearnROM/ROM_POD_separate_onV_theta\"+str(version)+\"N_m_v\"+str(list_Nv[i])+\"N_m_theta\"+str(list_Ntheta[i])+\".pt\"))\n",
    "    model.eval()\n",
    "    # making an initial condition\n",
    "    index_delete=np.array([])\n",
    "    if list_Nv[i]<N_m_load:\n",
    "        index_delete=np.append(index_delete,np.arange(list_Nv[i],N_m_load))\n",
    "    if list_Ntheta[i]<N_m_load:\n",
    "        index_delete=np.append(index_delete,np.arange(N_m_load+list_Ntheta[i],2*N_m_load))\n",
    "    xk=X_full[-1,:].reshape(1,N_m_load+N_m_load)/max_X_list[i] # We use the scaled coordinates\n",
    "        #removing the extra modes\n",
    "    index_delete = index_delete.astype(int)\n",
    "    xk=np.delete(xk,index_delete)\n",
    "    xk=torch.tensor(xk, dtype=torch.float32)\n",
    "    xk.to(device)\n",
    "    forecast=xk.detach().numpy().reshape(list_Nv[i]+list_Ntheta[i],1)\n",
    "    for index in range(N_it):\n",
    "        ykp1=(model(xk))*(max_y_list[i]/max_X_list[i])\n",
    "        xkp1=ykp1+xk\n",
    "        numpy_vector = xkp1.detach().numpy().reshape(list_Nv[i]+list_Ntheta[i],1)\n",
    "        forecast = np.hstack([forecast, numpy_vector])\n",
    "        xk=xkp1 \n",
    "    forecasts.append(forecast)\n",
    "    X_MLs.append(forecast.T*max_X_list[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the invariant measure in the pytorch model\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'text.usetex': True,  # If you want to use LaTeX for rendering text\n",
    "})\n",
    "fig, axs = plt.subplots(4,3, figsize=(8, 10))\n",
    "\n",
    "plt.rcParams.update({'font.family': 'serif', 'font.serif': 'Times New Roman','font.size': 7})\n",
    "\n",
    "\n",
    "# Flatten the axs array so that we can iterate over it easily\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot the KDE plot for each column of X_full\n",
    "for i in range(12):\n",
    "    sns.kdeplot(forecasts[0][i,:]*max_X_list[0], ax=axs[i],label='ROM (n=40)')\n",
    "    #sns.kdeplot(forecasts[1][i,:]*max_X_list[1], ax=axs[i],label='ROM (n=50)')\n",
    "    sns.kdeplot(forecasts[2][i,:]*max_X_list[2], ax=axs[i],label='ROM (n=60)')\n",
    "    sns.kdeplot(X_full[:, i], ax=axs[i],label='PDE',color='black')\n",
    "    axs[i].set_title(r'$\\alpha^v_{{{}}}$'.format(i+1))\n",
    "    axs[i].legend(frameon=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "# saving the figure in the central group directory\n",
    "fig.savefig(\"/central/groups/astuart/hkaveh/Figs/ROM/invariant_measure_v\"+\".pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the invariant measure in the pytorch model\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'text.usetex': True,  # If you want to use LaTeX for rendering text\n",
    "})\n",
    "fig, axs = plt.subplots(4,3, figsize=(8, 10))\n",
    "\n",
    "plt.rcParams.update({'font.family': 'serif', 'font.serif': 'Times New Roman','font.size': 7})\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot the KDE plot for each column of X_full\n",
    "for i in range(12):\n",
    "    sns.kdeplot(forecasts[0][i+list_Nv[0],:]*max_X_list[0], ax=axs[i],label='ROM (n=40)')\n",
    "    #sns.kdeplot(forecasts[1][i+list_Nv[1],:]*max_X_list[1], ax=axs[i],label='ROM (n=50)')\n",
    "    sns.kdeplot(forecasts[2][i+list_Nv[2],:]*max_X_list[2], ax=axs[i],label='ROM (n=60)')\n",
    "    sns.kdeplot(X_full[:, i+N_m_load], ax=axs[i],label='PDE',color='black')\n",
    "    axs[i].set_title(r'$\\alpha^\\theta_{{{}}}$'.format(i+1))\n",
    "    axs[i].legend(frameon=False)\n",
    "    print(i+list_Nv[0])\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"/central/groups/astuart/hkaveh/Figs/ROM/invariant_measure_theta\"+\".pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Step Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reduced order model for v is already loaded\n",
    "# lets load the model for dt\n",
    "# Loading the pytorch ML model\n",
    "# we had saved the model with the following code:\n",
    "version=2024.11\n",
    "#torch.save(model.state_dict(), \"/central/groups/astuart/hkaveh/Data/LearnROM/ROMdt_POD_separate_onV_theta\"+str(version)+\"N_m_v\"+str(N_m_v)+\".pt\")# now we want to load it\n",
    "# lets code the forward model\n",
    "# %% Defining model\n",
    "class Forwardmapdt(nn.Module):\n",
    "    def __init__(self,N_m):\n",
    "        super().__init__()\n",
    "        self.hidden1=nn.Linear(N_m+1,2*N_m)\n",
    "        self.act1=nn.Tanh()\n",
    "        self.hidden2=nn.Linear(2*N_m,4*N_m)\n",
    "        self.act2=nn.Tanh()\n",
    "        self.hidden3=nn.Linear(4*N_m,4*N_m)\n",
    "        self.act3=nn.Tanh()\n",
    "        self.hidden4=nn.Linear(4*N_m,2*N_m)\n",
    "        self.act4=nn.Tanh()\n",
    "        self.output=nn.Linear(2*N_m,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.act1(self.hidden1(x))\n",
    "        x=self.act2(self.hidden2(x))\n",
    "        x=self.act3(self.hidden3(x))\n",
    "        x=self.act4(self.hidden4(x))\n",
    "        x=(self.output(x))\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_logvmax(alpha,phi_v):\n",
    "    v=np.dot(phi_v,alpha)[:, np.newaxis]+q_bar_v # v and theta\n",
    "    logvmax=np.max(v)\n",
    "    return logvmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets scale the X_full and concatinate logvmax_reducedmodel into the input data in order to use modeldt\n",
    "max_X_dt=657.0901803962595 # these are the same for different list_Nv[i]\n",
    "max_Y_dt=7.330702809730826 # these are the same for different list_Nv[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logvmax_reducedmodels=np.zeros((X_full.shape[0],3))\n",
    "strings=['40','50','60']\n",
    "# initializing a figure with adjusting the rcParams\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'text.usetex': True,  # If you want to use LaTeX for rendering text\n",
    "})\n",
    "fig, axs = plt.subplots(1, 1, figsize=(3.7, 3))\n",
    "\n",
    "plt.rcParams.update({'font.family': 'serif', 'font.serif': 'Times New Roman','font.size': 8})\n",
    "\n",
    "\n",
    "for i in [0,2]:\n",
    "    print(i)\n",
    "    index_delete=np.array([])\n",
    "    if list_Nv[i]<N_m_load:\n",
    "        index_delete=np.append(index_delete,np.arange(list_Nv[i],N_m_load))\n",
    "    index_delete = index_delete.astype(int)\n",
    "    print(index_delete)\n",
    "    # removing colmns from X_full and phi\n",
    "    X_full_removed=np.delete(X_full,index_delete,axis=1) # this still contains theta\n",
    "    phi_removed=np.delete(phi,index_delete,axis=1)\n",
    "    \n",
    "    logvmax_reducedmodel=np.zeros((X_full_removed.shape[0],1))\n",
    "    for j in range(X_full_removed.shape[0]):\n",
    "        logvmax_reducedmodels[j,i]=find_logvmax(X_full_removed[j,:list_Nv[i]],phi_removed)   \n",
    "    X_full_scaled=X_full_removed[:,:list_Nv[i]]/max_X_dt\n",
    "    X_full_scaled = np.concatenate((X_full_scaled, logvmax_reducedmodels[:,i].reshape(X_full_scaled.shape[0],1)), axis=1)\n",
    "    Y_full_scaled=Y_full/max_Y_dt\n",
    "    X_cut=X_full_scaled[-N_cut:,:]\n",
    "    Y_cut=Y_full_scaled[-N_cut:,:]\n",
    "    \n",
    "    if i==0:\n",
    "        axs.plot(Y_cut[:,0]*max_Y_dt,label='True',linewidth=3) \n",
    "    X_cut_tensor = torch.tensor(X_cut, dtype=torch.float32)\n",
    "    index=np.linspace(1,N_cut-1,N_cut-1)\n",
    "    modeldt=Forwardmapdt(list_Nv[i])\n",
    "    modeldt.load_state_dict( torch.load(\"/central/groups/astuart/hkaveh/Data/LearnROM/ROMdt_POD_separate_onV_theta\"+str(version)+\"N_m_v\"+str(list_Nv[i])+\".pt\"))\n",
    "    modeldt.eval()\n",
    "    X_cut_tensor.to(device)\n",
    "    Pred=modeldt(X_cut_tensor)\n",
    "    axs.plot(Pred.detach().numpy()[:,0]*max_Y_dt,label=' n='+str(strings[i]))\n",
    "axs.set_xlabel('iteration')\n",
    "axs.set_ylabel(r'$\\log_{10}dt$')\n",
    "axs.legend(frameon=False)\n",
    "# saving the figure in the central group directory\n",
    "fig.savefig(\"/central/groups/astuart/hkaveh/Figs/ROM/draft_dt\"+\".pdf\", bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the scaling laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_ox[Start_index:,0,0]/t_yr,V_max_onesimulation)\n",
    "for i in range(TimeStarts.size):\n",
    "    plt.axvline(x=TimeStarts[i]/t_yr,color='r')\n",
    "    plt.axvline(x=TimeEnds[i]/t_yr,color='b')\n",
    "plt.axhline(y=np.log10(V_thresh),color='g')\n",
    "plt.xlim([215,225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(10**(1.5*(Mws+6)),T2s-T1s,'o',label='QDYN')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.plot(10**(1.5*(Mws_v2+6)),T2s_v2-T1s_v2,'o',label='QDYN updated method to find magnitude')  \n",
    "# plt.legend()\n",
    "# plt.title('Moment Duration Scaling Law')\n",
    "# plt.xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "# plt.ylabel(r\"duration (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the moment area scaling law\n",
    "plt.figure()\n",
    "Nrectangles=int(rectangles_all.size/4)\n",
    "rectangles_all=np.reshape(rectangles_all,(Nrectangles,4))\n",
    "Areas=W*(rectangles_all[:,3])*1000\n",
    "plt.plot(10**(1.5*(Mws_v2+6)),Areas,'o',label='QDYN updated method to find magnitude')  \n",
    "plt.title('Moment Area Scaling Law')\n",
    "plt.xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "plt.ylabel(r\"Area ($m^2$)\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logvmax_reducedmodels=np.zeros((N_it+1,3))\n",
    "times=np.zeros((N_it+1,3))\n",
    "# Finding the three dimensional velocities matrix from the ROM\n",
    "# make a figure with two subplots:\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'text.usetex': True,  # If you want to use LaTeX for rendering text\n",
    "})\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "plt.rcParams.update({'font.family': 'serif', 'font.serif': 'Times New Roman','font.size': 8})\n",
    "\n",
    "size=U_v.shape[0]\n",
    "size_x=256\n",
    "size_y=32\n",
    "V_ox_ROMs=[]\n",
    "t_ox_ROMs=[]\n",
    "for i in range(1):\n",
    "    # organize the input\n",
    "    X_input_partial=X_MLs[i] # xinput without the vmax\n",
    "    # finding vmax\n",
    "    for j in range(N_it+1):\n",
    "        logvmax_reducedmodels[j,i]=find_logvmax(X_input_partial[j,:list_Nv[i]],phi[:,:list_Nv[i]])    \n",
    "    X_ML_scaled=X_input_partial[:,:list_Nv[i]]/max_X_dt\n",
    "    X_ML_scaled = np.concatenate((X_ML_scaled, logvmax_reducedmodels[:,i].reshape(N_it+1,1)), axis=1)\n",
    "    # load model:\n",
    "    modeldt=Forwardmapdt(list_Nv[i])\n",
    "    modeldt.load_state_dict( torch.load(\"/central/groups/astuart/hkaveh/Data/LearnROM/ROMdt_POD_separate_onV_theta\"+str(version)+\"N_m_v\"+str(list_Nv[i])+\".pt\"))\n",
    "    modeldt.eval()\n",
    "    X_ML_scaled_tensor = torch.tensor(X_ML_scaled, dtype=torch.float32)\n",
    "    X_ML_scaled_tensor.to(device)\n",
    "    dt_ML=modeldt(X_ML_scaled_tensor)\n",
    "    dt_ML_detach=dt_ML.detach().numpy()\n",
    "    dt_ML_detach_scaled=dt_ML_detach*max_Y_dt\n",
    "    dt_ML_realtime=10**dt_ML_detach_scaled\n",
    "    time_ML=np.cumsum(dt_ML_realtime)\n",
    "    times[:,i]=time_ML\n",
    "    # plt.plot(dt_ML.detach().numpy(),label='Predicted')  \n",
    "    time_ML=time_ML.reshape(-1,1,1) # adding three dimension to time_ML\n",
    "    # \n",
    "    U_ROM=U_v[:,0:list_Nv[i]]\n",
    "    \n",
    "    V_ox_ROM=np.empty((N_it+1,size_y,size_x))\n",
    "    t_ox_ROM=np.ones((N_it+1,size_y,size_x))\n",
    "    #appending vmax\n",
    "    vmax=[]\n",
    "    for j in range(N_it+1):\n",
    "        V_snapshot = U_ROM @ (forecasts[i][:list_Nv[i], j] * max_X_list[i]) + q_bar_v.reshape(size,)\n",
    "        V_snapshot = V_snapshot[:size_x * size_y].reshape(size_y, size_x)\n",
    "        vmax.append(np.max(V_snapshot))\n",
    "        V_ox_ROM[j,:,:]=10**(V_snapshot)\n",
    "        t_ox_ROM[j,:,:]=time_ML[j]*np.ones((size_y,size_x))\n",
    "    V_ox_ROMs.append(V_ox_ROM)\n",
    "    t_ox_ROMs.append(t_ox_ROM)\n",
    "    TimeStarts_ML,TimeEnds_ML,rectangles_ML,Mags_ML=Find_T_X_tau_without_p_input(V_ox_ROM,t_ox_ROM,V_thresh,L_thresh,t_yr,x_ox,z_ox,L_fault,mu)\n",
    "    axs[0].plot(10**(1.5*(Mags_ML+6)),TimeEnds_ML-TimeStarts_ML,'o',label='ROM with n='+str(strings[i]),alpha=0.2)\n",
    "    Nrectangles_ML=int(rectangles_ML.size/4)\n",
    "    rectangles_ML=np.reshape(rectangles_ML,(Nrectangles_ML,4))\n",
    "    Areas_ML=W*(rectangles_ML[:,3])*1000\n",
    "    axs[1].plot(10**(1.5*(Mags_ML+6)),Areas_ML,'o',label='ROM with n='+str(strings[i]),alpha=0.2)\n",
    "\n",
    "\n",
    "axs[0].plot(10**(1.5*(Mws_v2+6)),T2s_v2-T1s_v2,'o',label='PDE',color='black',alpha=0.005)  \n",
    "axs[1].plot(10**(1.5*(Mws_v2+6)),Areas,'o',label='PDE',color='black',alpha=0.005)\n",
    "\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].legend()\n",
    "#axs[0].set_title('Moment Duration Scaling Law')\n",
    "axs[0].set_xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "axs[0].set_ylabel(r\"Duration($s$)\")\n",
    "\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].legend()\n",
    "#axs[1].set_title('Moment Area Scaling Law')\n",
    "axs[1].set_xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "axs[1].set_ylabel(r\"Area ($m^2$)\")\n",
    "# plot a line with slope 1/3 in log log scale\n",
    "x = np.linspace(10**(16),10**18,100)\n",
    "y = 10**(1/3*np.log10(x*1e5)) \n",
    "y2= 10**(2/3*np.log10(x*1e-3))\n",
    "axs[0].plot(x,y,color='black')\n",
    "axs[1].plot(x,y2,color='black')\n",
    "\n",
    "# adding text on the plot\n",
    "axs[0].text(10**17,10**7,'$T \\propto M^{1/3}$',rotation=20)\n",
    "axs[1].text(10**17,10**9,'$A \\propto M^{2/3}$',rotation=20)\n",
    "\n",
    "for ax in axs:\n",
    "\n",
    "    leg = ax.legend()\n",
    "    # Set legend handle alpha to 1 for each legend\n",
    "    for lh in leg.legendHandles:\n",
    "        lh.set_alpha(1)\n",
    "plt.tight_layout()\n",
    "#saving the figure in the central group directory\n",
    "fig.savefig(\"/central/groups/astuart/hkaveh/Figs/ROM/scaling_lawsN=40\"+\".pdf\", bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logvmax_reducedmodels=np.zeros((N_it+1,3))\n",
    "times=np.zeros((N_it+1,3))\n",
    "# Finding the three dimensional velocities matrix from the ROM\n",
    "# make a figure with two subplots:\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'text.usetex': True,  # If you want to use LaTeX for rendering text\n",
    "})\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "plt.rcParams.update({'font.family': 'serif', 'font.serif': 'Times New Roman','font.size': 8})\n",
    "\n",
    "size=U_v.shape[0]\n",
    "size_x=256\n",
    "size_y=32\n",
    "V_ox_ROMs=[]\n",
    "t_ox_ROMs=[]\n",
    "for i in range(1,2):\n",
    "    # organize the input\n",
    "    X_input_partial=X_MLs[i] # xinput without the vmax\n",
    "    # finding vmax\n",
    "    for j in range(N_it+1):\n",
    "        logvmax_reducedmodels[j,i]=find_logvmax(X_input_partial[j,:list_Nv[i]],phi[:,:list_Nv[i]])    \n",
    "    X_ML_scaled=X_input_partial[:,:list_Nv[i]]/max_X_dt\n",
    "    X_ML_scaled = np.concatenate((X_ML_scaled, logvmax_reducedmodels[:,i].reshape(N_it+1,1)), axis=1)\n",
    "    # load model:\n",
    "    modeldt=Forwardmapdt(list_Nv[i])\n",
    "    modeldt.load_state_dict( torch.load(\"/central/groups/astuart/hkaveh/Data/LearnROM/ROMdt_POD_separate_onV_theta\"+str(version)+\"N_m_v\"+str(list_Nv[i])+\".pt\"))\n",
    "    modeldt.eval()\n",
    "    X_ML_scaled_tensor = torch.tensor(X_ML_scaled, dtype=torch.float32)\n",
    "    X_ML_scaled_tensor.to(device)\n",
    "    dt_ML=modeldt(X_ML_scaled_tensor)\n",
    "    dt_ML_detach=dt_ML.detach().numpy()\n",
    "    dt_ML_detach_scaled=dt_ML_detach*max_Y_dt\n",
    "    dt_ML_realtime=10**dt_ML_detach_scaled\n",
    "    time_ML=np.cumsum(dt_ML_realtime)\n",
    "    times[:,i]=time_ML\n",
    "    # plt.plot(dt_ML.detach().numpy(),label='Predicted')  \n",
    "    time_ML=time_ML.reshape(-1,1,1) # adding three dimension to time_ML\n",
    "    # \n",
    "    U_ROM=U_v[:,0:list_Nv[i]]\n",
    "    \n",
    "    V_ox_ROM=np.empty((N_it+1,size_y,size_x))\n",
    "    t_ox_ROM=np.ones((N_it+1,size_y,size_x))\n",
    "    #appending vmax\n",
    "    vmax=[]\n",
    "    for j in range(N_it+1):\n",
    "        V_snapshot = U_ROM @ (forecasts[i][:list_Nv[i], j] * max_X_list[i]) + q_bar_v.reshape(size,)\n",
    "        V_snapshot = V_snapshot[:size_x * size_y].reshape(size_y, size_x)\n",
    "        vmax.append(np.max(V_snapshot))\n",
    "        V_ox_ROM[j,:,:]=10**(V_snapshot)\n",
    "        t_ox_ROM[j,:,:]=time_ML[j]*np.ones((size_y,size_x))\n",
    "    V_ox_ROMs.append(V_ox_ROM)\n",
    "    t_ox_ROMs.append(t_ox_ROM)\n",
    "    TimeStarts_ML,TimeEnds_ML,rectangles_ML,Mags_ML=Find_T_X_tau_without_p_input(V_ox_ROM,t_ox_ROM,V_thresh,L_thresh,t_yr,x_ox,z_ox,L_fault,mu)\n",
    "    axs[0].plot(10**(1.5*(Mags_ML+6)),TimeEnds_ML-TimeStarts_ML,'o',label='ROM with n='+str(strings[i]),alpha=0.2)\n",
    "    Nrectangles_ML=int(rectangles_ML.size/4)\n",
    "    rectangles_ML=np.reshape(rectangles_ML,(Nrectangles_ML,4))\n",
    "    Areas_ML=W*(rectangles_ML[:,3])*1000\n",
    "    axs[1].plot(10**(1.5*(Mags_ML+6)),Areas_ML,'o',label='ROM with n='+str(strings[i]),alpha=0.2)\n",
    "\n",
    "\n",
    "axs[0].plot(10**(1.5*(Mws_v2+6)),T2s_v2-T1s_v2,'o',label='PDE',color='black',alpha=0.005)  \n",
    "axs[1].plot(10**(1.5*(Mws_v2+6)),Areas,'o',label='PDE',color='black',alpha=0.005)\n",
    "\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].legend()\n",
    "#axs[0].set_title('Moment Duration Scaling Law')\n",
    "axs[0].set_xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "axs[0].set_ylabel(r\"Duration($s$)\")\n",
    "\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].legend()\n",
    "#axs[1].set_title('Moment Area Scaling Law')\n",
    "axs[1].set_xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "axs[1].set_ylabel(r\"Area ($m^2$)\")\n",
    "# plot a line with slope 1/3 in log log scale\n",
    "x = np.linspace(10**(16),10**18,100)\n",
    "y = 10**(1/3*np.log10(x*1e5)) \n",
    "y2= 10**(2/3*np.log10(x*1e-3))\n",
    "axs[0].plot(x,y,color='black')\n",
    "axs[1].plot(x,y2,color='black')\n",
    "\n",
    "# adding text on the plot\n",
    "axs[0].text(10**17,10**7,'$T \\propto M^{1/3}$',rotation=20)\n",
    "axs[1].text(10**17,10**9,'$A \\propto M^{2/3}$',rotation=20)\n",
    "\n",
    "for ax in axs:\n",
    "\n",
    "    leg = ax.legend()\n",
    "    # Set legend handle alpha to 1 for each legend\n",
    "    for lh in leg.legendHandles:\n",
    "        lh.set_alpha(1)\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"/central/groups/astuart/hkaveh/Figs/ROM/scaling_lawsN=50\"+\".pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logvmax_reducedmodels=np.zeros((N_it+1,3))\n",
    "times=np.zeros((N_it+1,3))\n",
    "# Finding the three dimensional velocities matrix from the ROM\n",
    "# make a figure with two subplots:\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'text.usetex': True,  # If you want to use LaTeX for rendering text\n",
    "})\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "plt.rcParams.update({'font.family': 'serif', 'font.serif': 'Times New Roman','font.size': 8})\n",
    "\n",
    "size=U_v.shape[0]\n",
    "size_x=256\n",
    "size_y=32\n",
    "V_ox_ROMs=[]\n",
    "t_ox_ROMs=[]\n",
    "for i in range(2,3):\n",
    "    # organize the input\n",
    "    X_input_partial=X_MLs[i] # xinput without the vmax\n",
    "    # finding vmax\n",
    "    for j in range(N_it+1):\n",
    "        logvmax_reducedmodels[j,i]=find_logvmax(X_input_partial[j,:list_Nv[i]],phi[:,:list_Nv[i]])    \n",
    "    X_ML_scaled=X_input_partial[:,:list_Nv[i]]/max_X_dt\n",
    "    X_ML_scaled = np.concatenate((X_ML_scaled, logvmax_reducedmodels[:,i].reshape(N_it+1,1)), axis=1)\n",
    "    # load model:\n",
    "    modeldt=Forwardmapdt(list_Nv[i])\n",
    "    modeldt.load_state_dict( torch.load(\"/central/groups/astuart/hkaveh/Data/LearnROM/ROMdt_POD_separate_onV_theta\"+str(version)+\"N_m_v\"+str(list_Nv[i])+\".pt\"))\n",
    "    modeldt.eval()\n",
    "    X_ML_scaled_tensor = torch.tensor(X_ML_scaled, dtype=torch.float32)\n",
    "    X_ML_scaled_tensor.to(device)\n",
    "    dt_ML=modeldt(X_ML_scaled_tensor)\n",
    "    dt_ML_detach=dt_ML.detach().numpy()\n",
    "    dt_ML_detach_scaled=dt_ML_detach*max_Y_dt\n",
    "    dt_ML_realtime=10**dt_ML_detach_scaled\n",
    "    time_ML=np.cumsum(dt_ML_realtime)\n",
    "    times[:,i]=time_ML\n",
    "    # plt.plot(dt_ML.detach().numpy(),label='Predicted')  \n",
    "    time_ML=time_ML.reshape(-1,1,1) # adding three dimension to time_ML\n",
    "    # \n",
    "    U_ROM=U_v[:,0:list_Nv[i]]\n",
    "    \n",
    "    V_ox_ROM=np.empty((N_it+1,size_y,size_x))\n",
    "    t_ox_ROM=np.ones((N_it+1,size_y,size_x))\n",
    "    #appending vmax\n",
    "    vmax=[]\n",
    "    for j in range(N_it+1):\n",
    "        V_snapshot = U_ROM @ (forecasts[i][:list_Nv[i], j] * max_X_list[i]) + q_bar_v.reshape(size,)\n",
    "        V_snapshot = V_snapshot[:size_x * size_y].reshape(size_y, size_x)\n",
    "        vmax.append(np.max(V_snapshot))\n",
    "        V_ox_ROM[j,:,:]=10**(V_snapshot)\n",
    "        t_ox_ROM[j,:,:]=time_ML[j]*np.ones((size_y,size_x))\n",
    "    V_ox_ROMs.append(V_ox_ROM)\n",
    "    t_ox_ROMs.append(t_ox_ROM)\n",
    "    TimeStarts_ML,TimeEnds_ML,rectangles_ML,Mags_ML=Find_T_X_tau_without_p_input(V_ox_ROM,t_ox_ROM,V_thresh,L_thresh,t_yr,x_ox,z_ox,L_fault,mu)\n",
    "    axs[0].plot(10**(1.5*(Mags_ML+6)),TimeEnds_ML-TimeStarts_ML,'o',label='ROM with n='+str(strings[i]),alpha=0.2)\n",
    "    Nrectangles_ML=int(rectangles_ML.size/4)\n",
    "    rectangles_ML=np.reshape(rectangles_ML,(Nrectangles_ML,4))\n",
    "    Areas_ML=W*(rectangles_ML[:,3])*1000\n",
    "    axs[1].plot(10**(1.5*(Mags_ML+6)),Areas_ML,'o',label='ROM with n='+str(strings[i]),alpha=0.2)\n",
    "\n",
    "\n",
    "axs[0].plot(10**(1.5*(Mws_v2+6)),T2s_v2-T1s_v2,'o',label='PDE',color='black',alpha=0.005)  \n",
    "axs[1].plot(10**(1.5*(Mws_v2+6)),Areas,'o',label='PDE',color='black',alpha=0.005)\n",
    "\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].legend()\n",
    "#axs[0].set_title('Moment Duration Scaling Law')\n",
    "axs[0].set_xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "axs[0].set_ylabel(r\"Duration($s$)\")\n",
    "\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].legend()\n",
    "#axs[1].set_title('Moment Area Scaling Law')\n",
    "axs[1].set_xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "axs[1].set_ylabel(r\"Area ($m^2$)\")\n",
    "# plot a line with slope 1/3 in log log scale\n",
    "x = np.linspace(10**(16),10**18,100)\n",
    "y = 10**(1/3*np.log10(x*1e5)) \n",
    "y2= 10**(2/3*np.log10(x*1e-3))\n",
    "axs[0].plot(x,y,color='black')\n",
    "axs[1].plot(x,y2,color='black')\n",
    "\n",
    "# adding text on the plot\n",
    "axs[0].text(10**17,10**7,'$T \\propto M^{1/3}$',rotation=20)\n",
    "axs[1].text(10**17,10**9,'$A \\propto M^{2/3}$',rotation=20)\n",
    "\n",
    "for ax in axs:\n",
    "\n",
    "    leg = ax.legend()\n",
    "    # Set legend handle alpha to 1 for each legend\n",
    "    for lh in leg.legendHandles:\n",
    "        lh.set_alpha(1)\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"/central/groups/astuart/hkaveh/Figs/ROM/scaling_lawsN=60\"+\".pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
