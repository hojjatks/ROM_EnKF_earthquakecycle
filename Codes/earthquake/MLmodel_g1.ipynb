{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "learn a reduced model for when you apply the POD seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ProcessFunctions import find_Aisv2,Find_a_i,find_Aisv2_onlyv\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns \n",
    "import cte_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_final=10500\n",
    "Ntout=cte_eq.Ntout\n",
    "Nxout=cte_eq.Nxout\n",
    "drs=0.012\n",
    "rho=13 # by looking at the plot in the FindDissipationBall.ipynb\n",
    "data_dir='/central/groups/astuart/hkaveh/Data/LearnROM/transfer/MainSimulation2D_Tf'+str(T_final)+\"Nt=\"+str(Ntout)+\"Nx=\"+str(Nxout)+'PODonlyonV'+'drs'+str(drs)+\".npz\"\n",
    "PODmodes=np.load(data_dir)\n",
    "U_v=PODmodes['U']\n",
    "S_v=PODmodes['S']\n",
    "# VT_v=PODmodes['VT']\n",
    "q_bar_v=PODmodes['q_bar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading POD modes\n",
    "\n",
    "\n",
    "data_dir='/central/groups/astuart/hkaveh/Data/LearnROM/transfer/MainSimulation2D_Tf'+str(T_final)+\"Nt=\"+str(Ntout)+\"Nx=\"+str(Nxout)+'PODonlyontheta'+'drs'+str(drs)+\".npz\"\n",
    "\n",
    "PODmodes=np.load(data_dir)\n",
    "U_theta=PODmodes['U']\n",
    "S_theta=PODmodes['S']\n",
    "# VT_theta=PODmodes['VT']\n",
    "q_bar_theta=PODmodes['q_bar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from \"/central/groups/astuart/hkaveh/Data/LearnROM/transfer/SampleSimulation_Tf_2D'+str(T_final_run)+\"Nt=\"+str(cte_eq.Ntout)+\"N_m\"+str(N_m)+\"coeff\"+str(coeff)+\"number\"+str(index)\"\n",
    "T_final_run = 350 # each initial condition is simulated for 250 years\n",
    "Nt=5     # it is recorded every Nt time stes\n",
    "N_m_v=20 # number of modes that they have considered\n",
    "N_m_theta=20 # number of modes that they have considered\n",
    "coeff=1 # How the inital conditions are from the chaotic attractor\n",
    "number=10 # Which one do you want to load\n",
    "N_cut=2000\n",
    "# loading time series:\n",
    "N_m_load=30\n",
    "\n",
    "X_full=np.empty((1,N_m_load*2))\n",
    "Y_full=np.empty((1,N_m_load*2))\n",
    "\n",
    "\n",
    "for number in range(1,101):\n",
    "    data_dir='/central/groups/astuart/hkaveh/Data/LearnROM/transfer/SampleSimulation_Tf_2D'+str(T_final_run)+\"Nt=\"+str(cte_eq.Ntout)+\"N_m\"+str(N_m_load)+\"coeff\"+str(coeff)+\"number\"+str(number)+\".npz\"\n",
    "    data_smaple=np.load(data_dir)\n",
    "\n",
    "    V_ox=data_smaple['array1']\n",
    "    theta_ox=data_smaple['array2']\n",
    "    t_ox=data_smaple['array3']\n",
    "    # removing data with probability p\n",
    "    xi=0\n",
    "    p_remove=np.tanh(xi*np.max(V_ox,axis=1))\n",
    "    Nt_sample=p_remove.shape[0]\n",
    "    print( \"number of points in this member is:\", Nt_sample)\n",
    "    keep_mask_sample=np.random.rand(Nt_sample) > p_remove\n",
    "    # We work with log10 of V_ox and theta_ox:\n",
    "    V_ox=np.log10(V_ox)\n",
    "    theta_ox=np.log10(theta_ox)\n",
    "    A_v,P_v=find_Aisv2_onlyv(U_v,V_ox,q_bar_v,N_m_load) # you dont need P in general, but I need it to check if I do everything correctly\n",
    "    A_theta,P_theta=find_Aisv2_onlyv(U_theta,theta_ox,q_bar_theta,N_m_load)\n",
    "    # concatenating the two arrays\n",
    "    A=np.concatenate((A_v,A_theta),axis=1)\n",
    "    X_onesimulation=A[:-1,:]         # current step\n",
    "    Y_onesimulation=A[1:,:]-A[:-1,:] # next step minus the current step\n",
    "    print(\"shape of X_onesimulation before is\",X_onesimulation.shape)\n",
    "    X_onesimulation=X_onesimulation[keep_mask_sample[:-1]]\n",
    "    Y_onesimulation=Y_onesimulation[keep_mask_sample[:-1]]\n",
    "    print(\"shape of X_onesimulation after is\",X_onesimulation.shape)\n",
    "\n",
    "    X_full=np.append(X_full,X_onesimulation,axis=0)\n",
    "    Y_full=np.append(Y_full,Y_onesimulation,axis=0)\n",
    "    print(\"number is \" +str(number)+ \" size of data in this simulation is \" +str(X_onesimulation.shape[0])+\" total size is \" + str(X_full.shape[0]))\n",
    "\n",
    "# removing the first row which is empty\n",
    "X_full=X_full[1:,:]\n",
    "Y_full=Y_full[1:,:]\n",
    "\n",
    "# print(np.max(X_full))\n",
    "print(np.max(Y_full))\n",
    "# Loading the the numpy arrays Input and Output which are generated for imposing dissipiation far away from the attractor\n",
    "# # commenting here:\n",
    "# data_dir=\"/central/groups/astuart/hkaveh/Data/LearnROM/EarthquakeEllipticShell_radi_outer\"+str(radi_outer)+\"radi_inner\"+str(radi_inner)+\"PODappliedseperately.npz\"\n",
    "\n",
    "# data_smaple=np.load(data_dir)\n",
    "# Input_X=data_smaple['Input']\n",
    "# Output_Y=data_smaple['Output']\n",
    "# # appending the Input_X to the X_full and Output_Y to the Y_full\n",
    "# # appending the Input_X to the X_full and Output_Y to the Y_full, but append from the begining of the array\n",
    "# print(np.max(Output_Y))\n",
    "# X_full=np.append(Input_X,X_full,axis=0)\n",
    "# Y_full=np.append(Output_Y,Y_full,axis=0)\n",
    "# # untill here\n",
    "\n",
    "# deleting some colomns from X_full and Y_full that are should not be included based on N_m_v and N_m_theta\n",
    "# index of the columns that should be deleted\n",
    "index_delete=np.array([])\n",
    "if N_m_v<N_m_load:\n",
    "    index_delete=np.append(index_delete,np.arange(N_m_v,N_m_load))\n",
    "if N_m_theta<N_m_load:\n",
    "    index_delete=np.append(index_delete,np.arange(N_m_load+N_m_theta,2*N_m_load))\n",
    "\n",
    "\n",
    "# Convert the array elements to integers\n",
    "index_delete = index_delete.astype(int)\n",
    "\n",
    "X_full=np.delete(X_full,index_delete,axis=1)\n",
    "Y_full=np.delete(Y_full,index_delete,axis=1)\n",
    "print(\"index of the columns that are deleted are:\")\n",
    "print(index_delete)\n",
    "\n",
    "print(X_full.shape)\n",
    "print(\"size of the data is \" +str(X_full.shape[0]))\n",
    "# 50, 55, 59, 63, 72, 73,79,81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Sigma_v and Sigma_theta and then divide the X_onesimulation by them and then find the norm and then plot it.\n",
    "# The directory is : directory='/central/groups/astuart/hkaveh/Data/LearnROM/transfer/'+'sigma'+str(T_final)+\"Nt=\"+str(Ntout)+\"Nx=\"+str(Nxout)+'PODonlyonV'+'drs'+str(drs)+\".npz\"\n",
    "# and they were saved using np.savez(directory,Sigma_v=Sigma_v,Sigma_theta=Sigma_theta,Nt2=Nt2)\n",
    "# Lets load them:\n",
    "N_plot=int(6e4)\n",
    "directory='/central/groups/astuart/hkaveh/Data/LearnROM/transfer/'+'sigma'+str(T_final)+\"Nt=\"+str(Ntout)+\"Nx=\"+str(Nxout)+'PODonlyonV'+'drs'+str(drs)+\".npz\"\n",
    "Sigma_v=np.load(directory)['Sigma_v']\n",
    "Sigma_theta=np.load(directory)['Sigma_theta']\n",
    "Sigma=np.concatenate((Sigma_v,Sigma_theta))\n",
    "X_full_scaled=X_full[:N_plot,:]/Sigma\n",
    "norm_X_full_scaled=np.linalg.norm(X_full_scaled,axis=1)\n",
    "# norm_Y_full_scaled=np.linalg.norm(Y_full_scaled,axis=1)\n",
    "plt.plot(norm_X_full_scaled)\n",
    "plt.axhline(y=rho, color='r', linestyle='-')\n",
    "# plt.plot(norm_Y_full_scaled)\n",
    "\n",
    "# plt.xlim(left=0,right=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "for i in range(10):\n",
    "    stat, p = shapiro(X_onesimulation[:, i])\n",
    "    print(f\"PC {i+1}: Shapiro p-value = {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "for i in range(10):\n",
    "    stat, p = shapiro(X_onesimulation[:, i])\n",
    "    print(f\"PC {i+1}: Shapiro p-value = {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_onesimulation[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_onesimulation[:,3])\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_onesimulation[:,0])\n",
    "# find v from alpha\n",
    "log_v=np.zeros((X_onesimulation.shape[0],1024))\n",
    "for i in range(X_onesimulation.shape[0]):\n",
    "    log_v[i,:]=U_v[:,:30]@X_onesimulation[i,:30]+q_bar_v.reshape(1024,)\n",
    "max_log_v=np.max(log_v,axis=1)\n",
    "plt.plot(max_log_v)\n",
    "# plt.xlim(right=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the  norm of X_full along each row\n",
    "norm_X_full=np.linalg.norm(X_full,axis=1)\n",
    "plt.hist(norm_X_full,bins=100)\n",
    "plt.title(\"Histogram of the norm of X_full along each row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis objects\n",
    "fig, axs = plt.subplots(3, 5, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axs array so that we can iterate over it easily\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot the KDE plot for each column of X_full\n",
    "for i in range(15):\n",
    "    sns.kdeplot(X_full[:, i], ax=axs[i])\n",
    "    axs[i].set_title(r'$\\alpha_{{{}}}$'.format(i))\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cut=X_full[-N_cut:,:]\n",
    "Y_cut=Y_full[-N_cut:,:]\n",
    "X=X_full[1:-N_cut,:] # Also, removing the first element\n",
    "Y=Y_full[1:-N_cut,:] \n",
    "# rescale\n",
    "# # comment from here:\n",
    "# max_X=np.max(X)\n",
    "# max_y=np.max(Y)\n",
    "\n",
    "# X=X/max_X\n",
    "# Y=Y/max_y\n",
    "# X_cut=X_cut/max_X\n",
    "# Y_cut=Y_cut/max_y\n",
    "# # untill here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target Y range:\")\n",
    "print(\"  Min:\", Y_train_tensor.min().item())\n",
    "print(\"  Max:\", Y_train_tensor.max().item())\n",
    "print(\"  Std:\", Y_train_tensor.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Defining model\n",
    "# class Forwardmap(nn.Module):\n",
    "#     def __init__(self,N_m,dropout_rate=0.2):\n",
    "#         super().__init__()\n",
    "#         self.hidden1=nn.Linear(N_m,2*N_m)\n",
    "#         self.act1=nn.Tanh()\n",
    "#         self.hidden2=nn.Linear(2*N_m,4*N_m)\n",
    "#         self.act2=nn.Tanh()\n",
    "#         self.hidden3=nn.Linear(4*N_m,4*N_m)\n",
    "#         self.act3=nn.Tanh()\n",
    "#         self.hidden4=nn.Linear(4*N_m,2*N_m)\n",
    "#         self.act4=nn.Tanh()\n",
    "#         self.output=nn.Linear(2*N_m,N_m)\n",
    "#         self.dropout = nn.Dropout(p=dropout_rate)\n",
    "#     def forward(self,x):\n",
    "#         x=self.act1(self.hidden1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x=self.act2(self.hidden2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x=self.act3(self.hidden3(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x=self.act4(self.hidden4(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x=(self.output(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "class Forwardmap(nn.Module):\n",
    "    def __init__(self, N_m, dropout_rate=0.2, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(N_m, 2 * N_m)\n",
    "        self.act1 = nn.LeakyReLU(negative_slope)\n",
    "        self.hidden2 = nn.Linear(2 * N_m, 4 * N_m)\n",
    "        self.act2 = nn.LeakyReLU(negative_slope)\n",
    "        self.hidden3 = nn.Linear(4 * N_m, 4 * N_m)\n",
    "        self.act3 = nn.LeakyReLU(negative_slope)\n",
    "        self.hidden4 = nn.Linear(4 * N_m, 2 * N_m)\n",
    "        self.act4 = nn.LeakyReLU(negative_slope)\n",
    "        self.output = nn.Linear(2 * N_m, N_m)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act4(self.hidden4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model=Forwardmap(N_m_v+N_m_theta)\n",
    "print(model)\n",
    "# Move the model to GPU\n",
    "model = model.to(device)\n",
    "# loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "loss_fn=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "# Define the scheduler for changing learning rate every 20 epochs\n",
    "lr_decay_factor = 0.5\n",
    "lr_scheduler_step = 50\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_scheduler_step, gamma=lr_decay_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize, save and evaluate the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=249\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs,targets in train_dataloader:\n",
    "        # Move inputs and targets to GPU\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=loss_fn(outputs,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(current_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "mean_loss = total_loss / len(test_dataset)\n",
    "print(f'Test Loss: {mean_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model in the group directory\n",
    "version=0\n",
    "torch.save(model.state_dict(), \"/central/groups/astuart/hkaveh/Data/LearnROM/Earthquake2D_ROM_POD_separate_onV_theta\"+str(version)+\"N_m_v\"+str(N_m_v)+\"N_m_theta\"+str(N_m_theta)+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cut_tensor = torch.tensor(X_cut, dtype=torch.float32)\n",
    "device = torch.device( \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "index=np.linspace(1,N_cut-1,N_cut-1)\n",
    "Pred=model(X_cut_tensor)\n",
    "plt.plot(Y_cut[:,0],label='True')\n",
    "plt.plot(Pred.detach().numpy()[:,0],label='Predicted')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel(r'$\\alpha_0(t_n+\\Delta t_n)-\\alpha_0(t_n)$')\n",
    "plt.xlim(left=0,right=2000)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cut_tensor = torch.tensor(X_cut, dtype=torch.float32)\n",
    "device = torch.device( \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "index=np.linspace(1,N_cut-1,N_cut-1)\n",
    "Pred=model(X_cut_tensor)\n",
    "plt.plot(Y_cut[:,0],label='True')\n",
    "plt.plot(Pred.detach().numpy()[:,0],label='Predicted')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel(r'$\\alpha_0(t_n+\\Delta t_n)-\\alpha_0(t_n)$')\n",
    "plt.xlim(left=0,right=500)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cut_tensor = torch.tensor(X_cut, dtype=torch.float32)\n",
    "device = torch.device( \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "index=np.linspace(1,N_cut-1,N_cut-1)\n",
    "Pred=model(X_cut_tensor)\n",
    "plt.plot(Y_cut[:,0],label='True')\n",
    "plt.plot(Pred.detach().numpy()[:,0],label='Predicted')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel(r'$\\alpha_0(t_n+\\Delta t_n)-\\alpha_0(t_n)$')\n",
    "plt.xlim(left=0,right=250)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cut_tensor = torch.tensor(X_cut, dtype=torch.float32)\n",
    "device = torch.device( \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "index=np.linspace(1,N_cut-1,N_cut-1)\n",
    "Pred=model(X_cut_tensor)\n",
    "plt.plot(Y_cut[:,0],label='True')\n",
    "plt.plot(Pred.detach().numpy()[:,0],label='Predicted')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel(r'$\\alpha_0(t_n+\\Delta t_n)-\\alpha_0(t_n)$')\n",
    "plt.xlim(left=0,right=250)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cut_tensor = torch.tensor(X_cut, dtype=torch.float32)\n",
    "device = torch.device( \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "index=np.linspace(1,N_cut-1,N_cut-1)\n",
    "Pred=model(X_cut_tensor)\n",
    "plt.plot(Y_cut[:,0],label='True')\n",
    "plt.plot(Pred.detach().numpy()[:,0],label='Predicted')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel(r'$\\alpha_0(t_n+\\Delta t_n)-\\alpha_0(t_n)$')\n",
    "plt.xlim(left=0,right=250)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta=5\n",
    "lambda_dissipation=0.9\n",
    "rho=10\n",
    "x_plot_test=np.linspace(0,2*rho,100)\n",
    "plt.plot(x_plot_test,1/(1+np.exp(Beta*(x_plot_test-rho))))\n",
    "plt.axvline(x=rho, color='r')\n",
    "plt.axvline(x=rho-1, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissipation_factor(xk,rho,Sigma,Beta=10):\n",
    "    u=xk/Sigma # normalize by the std\n",
    "    norm_u=torch.norm(u)\n",
    "    output=1/(1+torch.exp(Beta*(norm_u-rho)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma=np.concatenate((Sigma_v,Sigma_theta))\n",
    "Sigma = torch.tensor(Sigma, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xk=X_cut[0,:] # Picking one initial condition, this can be any point inside the chaotic attractor\n",
    "N_m=N_m_v+N_m_theta\n",
    "xk=torch.tensor(xk, dtype=torch.float32)\n",
    "forecast=xk.detach().numpy().reshape(N_m,1)\n",
    "for index in range(40*N_cut):\n",
    "    # ykp1=(model(xk))*(max_y/max_X)\n",
    "    dissipate=dissipation_factor(xk,rho,Sigma)\n",
    "    ykp1=(model(xk))\n",
    "    xkp1=(ykp1+xk)* dissipate + (1-dissipate)*(lambda_dissipation*xk) # equation 8 and 9 of the Learning Dissipative Dynamics in Chaotic Systems\n",
    "    numpy_vector = xkp1.detach().numpy().reshape(N_m,1)\n",
    "    forecast = np.hstack([forecast, numpy_vector])\n",
    "    xk=xkp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecast[0,:])\n",
    "plt.plot(X_full[:,0])\n",
    "plt.xlim(left=0,right=80000)\n",
    "# plt.ylim(bottom=-5000,top=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecast[1,:])\n",
    "plt.plot(X_full[:,1])\n",
    "\n",
    "plt.xlim(left=0,right=80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecast[4,:])\n",
    "plt.plot(X_full[:,4])\n",
    "plt.xlim(left=0,right=80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I want to plot two figures here. \n",
    "# # figure 1\n",
    "# # plot the time sereis of the norm of forecast\n",
    "# norm_forecast=np.linalg.norm(forecast,axis=0)\n",
    "# # I want to also grid the plot very fine\n",
    "# plt.figure(figsize=(10,5))\n",
    "# # set xlim\n",
    "# plt.xlim([0,1500])\n",
    "# plt.ylim([0,200])\n",
    "\n",
    "# # plt.plot(norm_forecast*max_X)\n",
    "# plt.plot(norm_forecast)\n",
    "\n",
    "# plt.title(\"Norm of the forecast\")\n",
    "# plt.xlabel(\"iteration\")\n",
    "# plt.ylabel(\"Norm of the forecast\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# alpha=forecast*max_X\n",
    "# alpha_v=alpha[:N_m_v,:]\n",
    "# alpha_theta=alpha[N_m_v:,:]\n",
    "# # Loading POD modes for V and Theta\n",
    "# N_m_v_load=30\n",
    "# N_m_theta_load=30\n",
    "# data_dir=\"/central/groups/astuart/hkaveh/Data/LearnROM/MainSimulation_Tf600Nt=5PODonlyonV.npz\"\n",
    "# PODmodes=np.load(data_dir)\n",
    "# U_v=PODmodes['U']\n",
    "# S_v=PODmodes['S']\n",
    "# VT_v=PODmodes['VT']\n",
    "# q_bar_v=PODmodes['q_bar']\n",
    "# Sigma_v=np.diagonal(S_v)\n",
    "# Nt2=VT_v.shape[0]\n",
    "# Lambda_v=Sigma_v**2/Nt2\n",
    "# Lambda_v=np.atleast_2d(Lambda_v[:N_m_v_load])\n",
    "# Sigma_v=(np.sqrt(Lambda_v))\n",
    "\n",
    "\n",
    "# data_dir=\"/central/groups/astuart/hkaveh/Data/LearnROM/MainSimulation_Tf600Nt=5PODonlyontheta.npz\"\n",
    "# PODmodes=np.load(data_dir)\n",
    "# U_theta=PODmodes['U']\n",
    "# S_theta=PODmodes['S']\n",
    "# VT_theta=PODmodes['VT']\n",
    "# q_bar_theta=PODmodes['q_bar']\n",
    "# Sigma_theta=np.diagonal(S_theta)\n",
    "# Nt2=VT_theta.shape[0]\n",
    "# Lambda_theta=Sigma_theta**2/Nt2\n",
    "# Lambda_theta=np.atleast_2d(Lambda_theta[:N_m_theta_load])\n",
    "# Sigma_theta=(np.sqrt(Lambda_theta))\n",
    "# Sigma_v=Sigma_v[0,:N_m_v]\n",
    "# Sigma_theta=Sigma_theta[0,:N_m_theta]\n",
    "# normalized_alpha_v=alpha_v/Sigma_v[:, np.newaxis]\n",
    "# normalized_alpha_theta=alpha_theta/Sigma_theta[:, np.newaxis]\n",
    " \n",
    " \n",
    "# apppend the normalized_alpha_v and normalized_alpha_theta\n",
    "# normalized_alpha=np.append(normalized_alpha_v,normalized_alpha_theta,axis=0)\n",
    "# normalized_alpha.shape\n",
    "# # plot the timeseries of the norm of the normalized_alpha\n",
    "# norm_normalized_alpha=np.linalg.norm(normalized_alpha,axis=0)\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.plot(norm_normalized_alpha)\n",
    "# plt.title(\"Norm of the normalized_alpha\")\n",
    "# plt.xlabel(\"iteration\")\n",
    "# plt.xlim([0,1500])\n",
    "# plt.ylim([0,100])\n",
    "\n",
    "# plt.ylabel(\"Norm of the normalized_alpha\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
