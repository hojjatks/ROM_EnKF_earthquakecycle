{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding model noise\n",
    "This code is extracted from `Enkf.ipynb` and is written to run only once and to save the Error matrix and Covariance matrix, so for the future uses I dont find the error every time.\n",
    "\n",
    "\n",
    "$dt$ is a very important constant in this piece of code, it changes the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('./..')\n",
    "import cte\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ProcessFunctions import find_Aisv2,Find_a_i,find_Aisv2_onlyv,Find_T_X_tau_without_p_input\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns \n",
    "from scipy import integrate\n",
    "import scipy\n",
    "import cte_eq\n",
    "\n",
    "# Loading POD modes\n",
    "T_final=10500\n",
    "Ntout=cte_eq.Ntout\n",
    "Nxout=cte_eq.Nxout\n",
    "drs=0.012\n",
    "mu=3e10\n",
    "\n",
    "data_dir='/central/groups/Avouac_lab/hkaveh/Data/LearnROM/2D/MainSimulation2D_Tf'+str(T_final)+\"Nt=\"+str(Ntout)+\"Nx=\"+str(Nxout)+'PODonlyonV'+'drs'+str(drs)+\".npz\"\n",
    "PODmodes=np.load(data_dir)\n",
    "U_v=PODmodes['U']\n",
    "S_v=PODmodes['S']\n",
    "# VT_v=PODmodes['VT']\n",
    "q_bar_v=PODmodes['q_bar']\n",
    "Sigma_v=np.diagonal(S_v)\n",
    "Nt2=127534 # This is the number os snapshots used to find the POD, it is fined in the RunForward2D.ipynb\n",
    "Lambda_v=Sigma_v**2/Nt2 # Covariance matrix\n",
    "\n",
    "\n",
    "data_dir='/central/groups/Avouac_lab/hkaveh/Data/LearnROM/2D/MainSimulation2D_Tf'+str(T_final)+\"Nt=\"+str(Ntout)+\"Nx=\"+str(Nxout)+'PODonlyontheta'+'drs'+str(drs)+\".npz\"\n",
    "PODmodes=np.load(data_dir)\n",
    "U_theta=PODmodes['U']\n",
    "S_theta=PODmodes['S']\n",
    "# VT_theta=PODmodes['VT']\n",
    "q_bar_theta=PODmodes['q_bar']\n",
    "Sigma_theta=np.diagonal(S_theta)\n",
    "# Nt2=VT_theta.shape[0]\n",
    "Lambda_theta=Sigma_theta**2/Nt2 # Covariance matrix\n",
    "N_m_load=30\n",
    "phi=U_v[:,:N_m_load]    # This contains the eigen mode for velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=3600*6          #                for SSEs I used             5*24*3600 \n",
    "gamma_ratio=0.0001*5 # variance of the observation noise is multiplied by the value\n",
    "sigma_kernel=2000 # in meters\n",
    "\n",
    "\n",
    "\n",
    "N_m_v_list=[20,25,30]\n",
    "N_m_theta_list=[20,25,30]\n",
    "index=0\n",
    "N_m_theta=N_m_theta_list[index]\n",
    "N_m_v=N_m_v_list[index]\n",
    "\n",
    "Lambda=np.append(Lambda_v[:N_m_v],Lambda_theta[:N_m_theta]) # Lambda is the covariance matrix\n",
    "Lambda=np.diag(Lambda)\n",
    "\n",
    "Gamma=np.diag(Lambda_v[:N_m_v])\n",
    "\n",
    "t_yr=365*24*3600           # number of seconds in a year\n",
    "Nx=1024\n",
    "L_thresh=1e3\n",
    "L=240e3\n",
    "L_fault=L\n",
    "x_ox=np.linspace(-L/2,L/2,Nx)\n",
    "L_element_x=L/Nx\n",
    "\n",
    "sigma_x=sigma_kernel/L_element_x # in number of elements\n",
    "\n",
    "Obs_linear=0\n",
    "inflation=1  # inflection of the model noise covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L_element_x)\n",
    "print(sigma_x)\n",
    "print(\"for SSE example sigma_x was 1.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise\n",
    "I start with noise because it is the easiest. What do you need to make noise? probably the standard deviation of the POD modes. Some assumption on the ratio of the noise. Ok, how can you get the standard deviation? I think I can just import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model noise as xi \\in R^n which is a normal distribution with mean 0 and std proportional to Lambda,\n",
    "# We take the covarinace matrix of the model noise (remember covariance as the unit standard deviation squared)\n",
    "# Here covariance matrix is Sigma\n",
    "\n",
    "# def xi_call2(Lambda,xi_ratio,m):\n",
    "#     xi=np.random.multivariate_normal(np.zeros(Lambda.shape[0]),xi_ratio*Lambda,m)\n",
    "#     return xi.T\n",
    "\n",
    "def xi_call(N_m,cov,m):\n",
    "    xi=np.random.multivariate_normal(np.zeros(N_m),cov,m)\n",
    "    return xi.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the observation noise as eta \\in R^d which is a normal distribution with mean 0 and std proportional to Lambda_y,\n",
    "def eta_call(Gamma,gamma_ratio,m):\n",
    "    eta=np.random.multivariate_normal(np.zeros(Gamma.shape[0]),gamma_ratio*Gamma,m)\n",
    "    return eta.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model from the code MLmodel8postprocess.ipynb\n",
    "# %% Defining model (g_1)\n",
    "class Forwardmap(nn.Module):\n",
    "    def __init__(self, N_m, dropout_rate=0.2, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(N_m, 2 * N_m)\n",
    "        self.act1 = nn.LeakyReLU(negative_slope)\n",
    "        self.hidden2 = nn.Linear(2 * N_m, 4 * N_m)\n",
    "        self.act2 = nn.LeakyReLU(negative_slope)\n",
    "        self.hidden3 = nn.Linear(4 * N_m, 4 * N_m)\n",
    "        self.act3 = nn.LeakyReLU(negative_slope)\n",
    "        self.hidden4 = nn.Linear(4 * N_m, 2 * N_m)\n",
    "        self.act4 = nn.LeakyReLU(negative_slope)\n",
    "        self.output = nn.Linear(2 * N_m, N_m)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act4(self.hidden4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "version=0\n",
    "    \n",
    "model=Forwardmap(N_m_v+N_m_theta)\n",
    "model.load_state_dict(torch.load(\n",
    "    \"/central/groups/astuart/hkaveh/Data/LearnROM/Earthquake2D_ROM_POD_separate_onV_theta\"\n",
    "    + str(version)\n",
    "    + \"N_m_v\"\n",
    "    + str(N_m_v)\n",
    "    + \"N_m_theta\"\n",
    "    + str(N_m_theta)\n",
    "    + \".pt\",\n",
    "    map_location=torch.device('cpu')\n",
    "))\n",
    "model.eval()\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reduced order model for v is already loaded\n",
    "# lets load the model for dt\n",
    "# Loading the pytorch ML model\n",
    "version_dt=0\n",
    "class Forwardmapdt(nn.Module):\n",
    "    def __init__(self,N_m_v, dropout_rate=0.2, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.hidden1=nn.Linear(N_m_v+1,2*N_m_v)\n",
    "        self.act1=nn.LeakyReLU(negative_slope)\n",
    "        self.hidden2=nn.Linear(2*N_m_v,4*N_m_v)\n",
    "        self.act2=nn.LeakyReLU(negative_slope)\n",
    "        self.hidden3=nn.Linear(4*N_m_v,4*N_m_v)\n",
    "        self.act3=nn.LeakyReLU(negative_slope)\n",
    "        self.hidden4=nn.Linear(4*N_m_v,2*N_m_v)\n",
    "        self.act4=nn.LeakyReLU(negative_slope)\n",
    "        self.output=nn.Linear(2*N_m_v,1)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.act1(self.hidden1(x))\n",
    "        x = self.dropout(x)\n",
    "        x=self.act2(self.hidden2(x))\n",
    "        x = self.dropout(x)\n",
    "        x=self.act3(self.hidden3(x))\n",
    "        x = self.dropout(x)\n",
    "        x=self.act4(self.hidden4(x))\n",
    "        x = self.dropout(x)\n",
    "        x=(self.output(x))\n",
    "        return x\n",
    "    \n",
    "modeldt=Forwardmapdt(N_m_v)\n",
    "modeldt.load_state_dict( torch.load(\"/central/groups/Avouac_lab/hkaveh/Data/LearnROM/2D/EQ_ROM_g2\"+str(version)+\"N_m_v\"+str(N_m_v)+\".pt\"))\n",
    "modeldt.eval()\n",
    "modeldt=modeldt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phi_torch     = torch.tensor(phi[:,:20], dtype=torch.float32, device=device)\n",
    "q_bar_v_torch = torch.tensor(q_bar_v, dtype=torch.float32, device=device)\n",
    "\n",
    "def find_logvmax(alpha):\n",
    "    if not isinstance(alpha, torch.Tensor):\n",
    "        alpha = torch.tensor(alpha, dtype=phi_torch.dtype, device=phi_torch.device)\n",
    "    v = torch.matmul(phi_torch, alpha).unsqueeze(1) + q_bar_v_torch\n",
    "    logvmax = torch.max(v).detach()  # Get the maximum value in v\n",
    "    return logvmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_f=[N_m_v+N_m_theta]\n",
    "rho=2\n",
    "lambda_dissipation=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissipation_factor(xk,rho,Sigma,Beta=20):\n",
    "    # u=xk/Sigma # normalize by the std\n",
    "    alpha=xk[:,:N_m_v]\n",
    "    logvmax=find_logvmax(alpha.T)\n",
    "    # norm_u=torch.norm(u)\n",
    "    output=1/(1+torch.exp(Beta*(logvmax-rho)))\n",
    "    return output\n",
    "directory='/central/groups/Avouac_lab/hkaveh/Data/LearnROM/2D/sigma'+str(T_final)+\"Nt=\"+str(Ntout)+\"Nx=\"+str(Nxout)+'PODonlyonV'+'drs'+str(drs)+\".npz\"\n",
    "Sigma_v=np.load(directory)['Sigma_v']\n",
    "Sigma_theta=np.load(directory)['Sigma_theta']\n",
    "Sigma=np.concatenate((Sigma_v,Sigma_theta))\n",
    "Sigma = torch.tensor(Sigma, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is written to find the next time step in the Kalman filter algorithm and is the forward model\n",
    "# This function needs to be double checked.\n",
    "def f(alpha_k,dt,params_f):\n",
    "    # alpha_k is the state at time k\n",
    "    # alpha_kp1 is the state at time k+1\n",
    "    # dt is the time step (assume in seconds)\n",
    "    N_m=params_f[0]\n",
    "    xk=torch.tensor(alpha_k, dtype=torch.float32)           # xk is in torch.\n",
    "    forecast=xk.detach().cpu().numpy().reshape(N_m,1)       # forecast is in cpu.\n",
    "    time=np.array([0])                                      # time is in cpu\n",
    "    while time[-1]<dt:\n",
    "        xk=xk.to(device,dtype=torch.float32)\n",
    "        dissipate=dissipation_factor(xk.reshape((1,N_m)),rho,Sigma)          # torch\n",
    "        ykp1=(model(xk))                                    # ykp1 is the prediction of the model in torch\n",
    "        xkp1=(ykp1+xk)* dissipate + (1-dissipate)*(lambda_dissipation*xk) # This is the prediction of the model in torch\n",
    "\n",
    "        numpy_vector = xkp1.detach().cpu().numpy().reshape(N_m,1)\n",
    "        x_ML=numpy_vector                             # This is the alpha that has both v and theta\n",
    "        vmax=find_logvmax(x_ML[:N_m_v])       # This is the maximum of v\n",
    "\n",
    "        x_input_dt_ML=x_ML[:N_m_v]                 # This is the input for the dt model (except vmax)\n",
    "        x_input_dt_ML=np.append(x_input_dt_ML,vmax.detach().cpu().numpy())         # This is the input for the dt model\n",
    "        x_input_dt_ML=torch.tensor(x_input_dt_ML, dtype=torch.float32) # This is the input for the dt model\n",
    "        x_input_dt_ML=x_input_dt_ML.to(device)\n",
    "        dt_pred=modeldt(x_input_dt_ML)                      # This is the prediction for the time step\n",
    "        dt_pred=10 ** dt_pred.detach().cpu().double().numpy().astype(np.float64)    # This is the prediction for the time step\n",
    "        time=np.append(time,dt_pred+time[-1])                        # This is the time vector\n",
    "        forecast = np.hstack([forecast, numpy_vector])      # This is the forecasted alpha (not scaled though)\n",
    "        xk=xkp1\n",
    "    \n",
    "    # applying linear interpolation to find forecast at time dt, using only the last two points\n",
    "    f2 = interp1d([time[-2],time[-1]], forecast[:,-2:],axis=1)\n",
    "    alpha_kp1=f2(dt).reshape(N_m,1)\n",
    "    # Removing the last coulmn of forecast becasue it is for the time when t > dt\n",
    "    forecast=forecast[:,:-1]\n",
    "    time[-1]=dt\n",
    "    # Then appending it to the forecast\n",
    "    forecast = np.concatenate((forecast, alpha_kp1),axis=1)\n",
    "    return forecast,time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did it untill here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform time stepping discrete map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "* Import and load the model snapshot and timestep.\n",
    "* pick an initial condition. randomly. I tested the code with a few initial conditiions and checked the invariant measure and they look similar to the invariant measure of the system.\n",
    "* This is what I am going to do. In each iteration, I find the forecast, then multiply it by max_X to get alpha without scales. Then I find the vmax from the unscaled data. Then I divide alpha by `max_X_dt` and concatinate with vmax to have the input for the machine learning model of dt.  Then I run it through the ML model for time stepping and then to get the actual dt, I need to multiply the output by max_Y_dt.\n",
    "* Write the thing in a function. in the form: $\\alpha(t_{n+1})=f(\\alpha(t_n),t_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function is written to find the next time step in the Kalman filter algorithm and is the forward model\n",
    "# def f(alpha_k,dt,params_f):\n",
    "#     # alpha_k is the state at time k\n",
    "#     # alpha_kp1 is the state at time k+1\n",
    "#     # dt is the time step (assume in seconds)\n",
    "#     max_X=params_f[0]\n",
    "#     max_y=params_f[1]\n",
    "#     max_X_dt=params_f[2]\n",
    "#     max_Y_dt=params_f[3]\n",
    "#     N_m_v=params_f[4]\n",
    "#     phi=params_f[5]\n",
    "#     N_m=params_f[6]\n",
    "#     xk=torch.tensor(alpha_k/max_X, dtype=torch.float32)\n",
    "#     forecast=xk.detach().numpy().reshape(N_m,1)\n",
    "#     time=np.array([0])\n",
    "#     while time[-1]<dt:\n",
    "#         xk.to(device)\n",
    "#         ykp1=(model(xk))*(max_y/max_X)\n",
    "#         xkp1=ykp1+xk\n",
    "#         xk=xkp1\n",
    "#         numpy_vector = xkp1.detach().numpy().reshape(N_m,1)\n",
    "#         x_ML=numpy_vector*max_X                             # This is the alpha that has both v and theta\n",
    "#         vmax=find_logvmax(x_ML[:N_m_v],phi[:,:N_m_v])       # This is the maximum of v\n",
    "#         x_input_dt_ML=x_ML[:N_m_v]/max_X_dt                 # This is the input for the dt model (except vmax)\n",
    "#         x_input_dt_ML=np.append(x_input_dt_ML,vmax)         # This is the input for the dt model\n",
    "#         x_input_dt_ML=torch.tensor(x_input_dt_ML, dtype=torch.float32) # This is the input for the dt model\n",
    "#         x_input_dt_ML.to(device)\n",
    "#         dt_pred=modeldt(x_input_dt_ML)                      # This is the prediction for the time step\n",
    "#         dt_pred=10**(dt_pred.detach().numpy()*max_Y_dt)     # This is the prediction for the time step\n",
    "#         time=np.append(time,dt_pred+time[-1])                        # This is the time vector\n",
    "#         forecast = np.hstack([forecast, numpy_vector])      # This is the forecasted alpha (not scaled though)\n",
    "#     # applying linear interpolation to find forecast at time dt, using only the last two points\n",
    "#     f = interp1d([time[-2],time[-1]], forecast[:,-2:],axis=1)\n",
    "#     alpha_kp1=f(dt).reshape(N_m,1)\n",
    "#     # Removing the last coulmn of forecast becasue it is for the time when t > dt\n",
    "#     forecast=forecast[:,:-1]\n",
    "#     time[-1]=dt\n",
    "#     # Then appending it to the forecast\n",
    "#     forecast = np.concatenate((forecast, alpha_kp1),axis=1)*max_X\n",
    "#     return forecast,time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the forwawrd model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the forward model for a long time (to check the statistics):\n",
    "To plot (the scaling and invariant measure) you can uncomment the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell runs the program for up to time dt\n",
    "# dt=100*24*3600*365\n",
    "# size_x=256\n",
    "# size_y=32\n",
    "# size=U_v.shape[0]\n",
    "# alpha_k=xi_call(40,Lambda,1).T\n",
    "# forecast,time=f(alpha_k,dt,params_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell calculate the things we need for finding the scaling laws.\n",
    "# vmaxs=[]\n",
    "# V_ox_ROM=np.empty((forecast.shape[1],size_y,size_x))\n",
    "# t_ox_ROM=np.ones((forecast.shape[1],size_y,size_x))\n",
    "# U_ROM=U_v[:,0:N_m_v]\n",
    "\n",
    "# for j in range(forecast.shape[1]):\n",
    "#     V_snapshot = U_ROM @ (forecast[:N_m_v, j]) + q_bar_v.reshape(size,)\n",
    "#     V_snapshot = V_snapshot[:size_x * size_y].reshape(size_y, size_x)\n",
    "#     V_ox_ROM[j,:,:]=10**(V_snapshot)\n",
    "#     t_ox_ROM[j,:,:]=time[j]*np.ones((size_y,size_x))\n",
    "#     vmaxs.append(np.max(V_snapshot))\n",
    "# TimeStarts_ML,TimeEnds_ML,rectangles_ML,Mags_ML=Find_T_X_tau_without_p_input(V_ox_ROM,t_ox_ROM,V_thresh,L_thresh,t_yr,x_ox,z_ox,L_fault,mu)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading QDYN simulation and scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def FindMwupdated(V_full,t_full,V_thresh):\n",
    "#     flag=0\n",
    "#     V_max=np.max(V_full,axis=(1,2))\n",
    "#     Mw=np.array([])\n",
    "#     T1=np.array([]) # it is the time of when the earthquakes nucleate\n",
    "#     T2=np.array([]) # it is the time of when the earthquake stops\n",
    "#     PotRate=np.sum(V_full,axis=(1,2))*Area/Nx/Nz\n",
    "\n",
    "#     for i in range(V_max.size):\n",
    "        \n",
    "#         if flag==0 and V_max[i]>V_thresh: # an event has started\n",
    "#             flag=1\n",
    "#             index1=i\n",
    "#             T1=np.append(T1,t_full[i])\n",
    "#         if flag==1 and V_max[i]<V_thresh: # the event has stopped\n",
    "#             flag=0\n",
    "#             index2=i\n",
    "#             IntPotRate=integrate.cumtrapz(PotRate[index1:index2+1],t_full[index1:index2+1])\n",
    "#             Integration=IntPotRate[-1]\n",
    "#             M0=Integration*mu\n",
    "#             Mw=np.append(Mw,(2/3)*np.log10(M0)-6)\n",
    "#             T2=np.append(T2,t_full[i])\n",
    "#     return Mw,T1,T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I need to load some of the already run models to check the scaling, after checking you can commentout this cell\n",
    "# # Loading the data from the simulation\n",
    "# # Loading data from \"/central/groups/astuart/hkaveh/Data/LearnROM/\"\n",
    "# Tf = 250 # each initial condition is simulated for 250 years\n",
    "# Nt=5     # it is recorded every Nt time stes\n",
    "\n",
    "# N_m_load=30\n",
    "# coeff=2 # How the inital conditions are from the chaotic attractor\n",
    "# N_cut=2000\n",
    "# filter_ratio=0.4 # removing 40 percent of the data\n",
    "# # loading time series:\n",
    "\n",
    "\n",
    "# Mws_v2=np.array([])\n",
    "# T1s_v2=np.array([])\n",
    "# T2s_v2=np.array([])\n",
    "# rectangles_all=np.array([])\n",
    "\n",
    "# findscaling=1\n",
    "# X_full=np.empty((1,N_m_load*2))\n",
    "\n",
    "# for number in range(10):\n",
    "#     data_dir=\"/central/groups/astuart/hkaveh/Data/LearnROM/SampleSimulation_Tf\"+str(Tf)+\"Nt=\"+str(Nt)+\"N_m\"+str(N_m_load)+\"coeff\"+str(coeff)+\"number\"+str(number)+\".npz\"\n",
    "#     data_smaple=np.load(data_dir)\n",
    "#     V_ox=data_smaple['array1']\n",
    "#     theta_ox=data_smaple['array2']\n",
    "#     t_ox=data_smaple['array3']\n",
    "#     # We work we log10 of V_ox:\n",
    "#     t=t_ox[:,0,0].reshape(-1,1)\n",
    "#     Start_index=int(V_ox.shape[0]*filter_ratio)\n",
    "    \n",
    "\n",
    "#     if findscaling==1:\n",
    "\n",
    "#         TimeStarts,TimeEnds,rectangles,Mags=Find_T_X_tau_without_p_input(V_ox,t_ox,V_thresh,L_thresh,t_yr,x_ox,z_ox,L_fault,mu)\n",
    "#         Mws_v2=np.append(Mws_v2,Mags)\n",
    "#         T1s_v2=np.append(T1s_v2,TimeStarts)\n",
    "#         T2s_v2=np.append(T2s_v2,TimeEnds)\n",
    "#         rectangles_all=np.append(rectangles_all,rectangles)\n",
    "#     V_ox=np.log10(V_ox)\n",
    "#     theta_ox=np.log10(theta_ox)\n",
    "#     A_v,P_v=find_Aisv2_onlyv(U_v,V_ox,q_bar_v,N_m_load) # you dont need P in general, but I need it to check if I do everything correctly\n",
    "#     A_theta,P_theta=find_Aisv2_onlyv(U_theta,theta_ox,q_bar_theta,N_m_load)\n",
    "#     A=np.concatenate((A_v,A_theta),axis=1)\n",
    "#     X_onesimulation=A[Start_index:-1,:] # with removig the first 20 percent of the data to remove the transient\n",
    "#     X_full=np.append(X_full,X_onesimulation,axis=0)\n",
    "# X_full=X_full[1:,:]\n",
    "\n",
    "# Nrectangles=int(rectangles_all.size/4)\n",
    "# rectangles_all=np.reshape(rectangles_all,(Nrectangles,4))\n",
    "# Areas=W*(rectangles_all[:,3])*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking the invariant measure in the pytorch model\n",
    "\n",
    "# fig, axs = plt.subplots(3, 5, figsize=(15, 10))\n",
    "\n",
    "# # Flatten the axs array so that we can iterate over it easily\n",
    "# axs = axs.flatten()\n",
    "\n",
    "# # Plot the KDE plot for each column of X_full\n",
    "# for i in range(15):\n",
    "#     sns.kdeplot(forecast[i,:], ax=axs[i],label='ROM (n=40)')\n",
    "#     sns.kdeplot(X_full[:, i], ax=axs[i],label='QDYN')\n",
    "#     axs[i].set_title(r'$\\alpha_{{{}}}$'.format(i+1))\n",
    "#     axs[i].legend()\n",
    "\n",
    "\n",
    "# # Adjust layout and show plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the scaling laws:\n",
    "# plt.rcParams.update({\n",
    "#     'font.family': 'serif',\n",
    "#     'font.serif': ['Times New Roman'],\n",
    "#     'text.usetex': True,  # If you want to use LaTeX for rendering text\n",
    "# })\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# plt.rcParams.update({'font.family': 'serif', 'font.serif': 'Times New Roman','font.size': 8})\n",
    "# string=str(N_m)\n",
    "# axs[0].plot(10**(1.5*(Mags_ML+6)),TimeEnds_ML-TimeStarts_ML,'o',label='ROM with n='+str(string),alpha=0.2)\n",
    "# Nrectangles_ML=int(rectangles_ML.size/4)\n",
    "# rectangles_ML=np.reshape(rectangles_ML,(Nrectangles_ML,4))\n",
    "# Areas_ML=W*(rectangles_ML[:,3])*1000\n",
    "# axs[1].plot(10**(1.5*(Mags_ML+6)),Areas_ML,'o',label='ROM with n='+str(string),alpha=0.2)\n",
    "# axs[0].plot(10**(1.5*(Mws_v2+6)),T2s_v2-T1s_v2,'o',label='QDYN',color='black',alpha=0.005)  \n",
    "# axs[1].plot(10**(1.5*(Mws_v2+6)),Areas,'o',label='QDYN',color='black',alpha=0.005)\n",
    "\n",
    "# axs[0].set_xscale('log')\n",
    "# axs[0].set_yscale('log')\n",
    "# axs[0].legend()\n",
    "# axs[0].set_title('Moment Duration Scaling Law')\n",
    "# axs[0].set_xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "# axs[0].set_ylabel(r\"Duration($s$)\")\n",
    "\n",
    "# axs[1].set_xscale('log')\n",
    "# axs[1].set_yscale('log')\n",
    "# axs[1].legend()\n",
    "# axs[1].set_title('Moment Area Scaling Law')\n",
    "# axs[1].set_xlabel(r\"$\\int_{t_1}^{t_2} \\mu \\dot{P} dt$\")\n",
    "# axs[1].set_ylabel(r\"Area ($m^2$)\")\n",
    "# # plot a line with slope 1/3 in log log scale\n",
    "# x = np.linspace(10**(16),10**18,100)\n",
    "# y = 10**(1/3*np.log10(x*1e5)) \n",
    "# y2= 10**(2/3*np.log10(x*1e-3))\n",
    "# axs[0].plot(x,y,color='black')\n",
    "# axs[1].plot(x,y2,color='black')\n",
    "# for ax in axs:\n",
    "\n",
    "#     leg = ax.legend()\n",
    "#     # Set legend handle alpha to 1 for each legend\n",
    "#     for lh in leg.legendHandles:\n",
    "#         lh.set_alpha(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that I have checked everything, I can wrte function f for uniform time stepping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the interpolation:\n",
    "we start from an initial conditon, simulate it for 7 days and then simulate with the same initial conditions for 1 day, 2 days, 3 days 4 days and 5 days, then I plot how does the system look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast1,time1=f(alpha_k,1*24*3600,params_f)\n",
    "# forecast2,time2=f(alpha_k,2*24*3600,params_f)\n",
    "# forecast3,time3=f(alpha_k,3*24*3600,params_f)\n",
    "# forecast4,time4=f(alpha_k,4*24*3600,params_f)\n",
    "# forecast5,time5=f(alpha_k,5*24*3600,params_f)\n",
    "# forecast6,time6=f(alpha_k,6*24*3600,params_f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(time1/24/3600,forecast1[0,:],color='r',linewidth=10)    \n",
    "# plt.plot(time2/24/3600,forecast2[0,:],color='b',linewidth=8)\n",
    "# plt.plot(time3/24/3600,forecast3[0,:],color='g',linewidth=6)\n",
    "# plt.plot(time4/24/3600,forecast4[0,:],color='y',linewidth=4)\n",
    "# plt.plot(time5/24/3600,forecast5[0,:],color='k',linewidth=2)\n",
    "# plt.plot(time6/24/3600,forecast6[0,:],color='m',linewidth=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another double check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt/3600/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "800/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating model noise\n",
    "To find the model noise, I need `X_uniform`, `time_uniform` from the PDE then I need a function that gets `X_uniform(k)` and spits out X_uniform(k+1). I need to find this in a loop and try to find the error that comes from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: finding X_uniform and time_uniform\n",
    "# I need to load some of the already run models to check the scaling, after checking you can commentout this cell\n",
    "# Loading the data from the simulation\n",
    "# Loading data from \"/central/groups/astuart/hkaveh/Data/LearnROM/\"\n",
    "T_final_run = 350 # each initial condition is simulated for 250 years\n",
    "Nt=5     # it is recorded every Nt time stes\n",
    "N_m_v=20 # number of modes that they have considered\n",
    "N_m_theta=20 # number of modes that they have considered\n",
    "N_m_load=30\n",
    "coeff=1 # How the inital conditions are from the chaotic attractor\n",
    "N_cut=2000\n",
    "filter_ratio=0.5 # I think I should not remove anything, because the error should include data points outside the attractor as well because the particles will end up outside the attractor at some point.\n",
    "Errors=np.empty((1,N_m_v+N_m_theta))\n",
    "\n",
    "# loading time series:\n",
    "for number in range(10):\n",
    "    data_dir='/central/groups/Avouac_lab/hkaveh/Data/LearnROM/2D/SampleSimulation_Tf_2D'+str(T_final_run)+\"Nt=\"+str(cte_eq.Ntout)+\"N_m\"+str(N_m_load)+\"coeff\"+str(coeff)+\"number\"+str(number)+\".npz\"\n",
    "    data_smaple=np.load(data_dir)    \n",
    "    V_ox=data_smaple['array1']\n",
    "    theta_ox=data_smaple['array2']\n",
    "    t_ox=data_smaple['array3']\n",
    "    t=t_ox[:,0].reshape(-1,1)\n",
    "    Start_index=int(V_ox.shape[0]*filter_ratio)\n",
    "    V_ox=np.log10(V_ox)\n",
    "    theta_ox=np.log10(theta_ox)\n",
    "    A_v,P_v=find_Aisv2_onlyv(U_v,V_ox,q_bar_v,N_m_load) # you dont need P in general, but I need it to check if I do everything correctly\n",
    "    A_theta,P_theta=find_Aisv2_onlyv(U_theta,theta_ox,q_bar_theta,N_m_load)\n",
    "    A=np.concatenate((A_v,A_theta),axis=1)\n",
    "    X_onesimulation=A[Start_index:-1,:] # with removig the first Start_index data points\n",
    "    time_nonuniform=t[Start_index:-1,:]\n",
    "    print(\"shape X_onesimulation\",X_onesimulation.shape)\n",
    "    print(\"shape X_onesimulation\",time_nonuniform.shape)\n",
    "    t_0=time_nonuniform[0]\n",
    "    t_end=time_nonuniform[-1]\n",
    "    days=int((t_end-t_0)/24/3600)\n",
    "    index_delete=np.array([])\n",
    "    if N_m_v<N_m_load:\n",
    "        index_delete=np.append(index_delete,np.arange(N_m_v,N_m_load))\n",
    "    if N_m_theta<N_m_load:\n",
    "        index_delete=np.append(index_delete,np.arange(N_m_load+N_m_theta,2*N_m_load))\n",
    "\n",
    "    # # Convert the array elements to integers\n",
    "    index_delete = index_delete.astype(int)\n",
    "\n",
    "    X_onesimulation=np.delete(X_onesimulation,index_delete,axis=1)\n",
    "    print(X_onesimulation.shape)\n",
    "    print(time_nonuniform.shape)\n",
    "    #make uniform time steps strating from t_0\n",
    "    N_data=int((t_end-t_0)/dt)-1\n",
    "    time_uniform=np.linspace(t_0,t_0+N_data*dt,N_data)\n",
    "    X_uniform=np.empty((N_data,X_onesimulation.shape[1]))\n",
    "    for i in range(X_onesimulation.shape[1]):\n",
    "        X_uniform[:,i]=np.interp(time_uniform,time_nonuniform[:,0],X_onesimulation[:,i]).reshape(-1)\n",
    "\n",
    "    # writing the loop:\n",
    "    # I need a function (f) that gets `X_uniform(k)` and spits out X_uniform(k+1). I need to find this in a loop and try to find the error that comes from it.\n",
    "    \n",
    "    forecast=np.zeros((N_data,X_onesimulation.shape[1]))\n",
    "    for i in range(N_data):\n",
    "        forecast[i,:] =f(X_uniform[i,:],dt,params_f)[0][:,-1]\n",
    "    error=forecast[:-1,:]-X_uniform[1:,:]\n",
    "    plt.figure()\n",
    "    plt.plot(X_uniform[1:,0],color='red')\n",
    "    plt.plot(forecast[:-1,0],color='blue')\n",
    "    \n",
    "    Errors=np.append(Errors,error,axis=0)\n",
    "\n",
    "# # removing the first row of Errors\n",
    "Errors=Errors[1:,:]\n",
    "plt.plot(Errors[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecast[:,0],linewidth=5)\n",
    "\n",
    "plt.plot(X_uniform[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(Errors[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving Errors in Data\n",
    "np.savez(\"/central/groups/Avouac_lab/hkaveh/Data/LearnROM/2D/ML_EQ_Errors_N_m_v\"+str(N_m_v)+\"N_m_theta\"+str(N_m_theta)+\"version\"+str(version),Errors=Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting forecast(i) against X_uniform(i+1), check if I have found the error correctly\n",
    "# Ndum=6500\n",
    "# plt.plot(time_uniform[1:Ndum+1]/24/3600/365,forecast[:Ndum,0],label='Forecast',color='b',linestyle='None',marker='o',markersize=2)\n",
    "# plt.plot(time_uniform[1:Ndum+1]/24/3600/365,X_uniform[1:Ndum+1,0],label='X',color='r',linestyle='None',marker='o',markersize=2)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the array containing the error\n",
    "# error=forecast[:-1,:]-X_uniform[1:,:]\n",
    "# plt.plot(error,label='Error',color='g',linestyle='None',marker='o',markersize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean and std of the error along the columns\n",
    "mean_error=np.mean(Errors,axis=0)\n",
    "\n",
    "cov_matrix = np.cov(Errors-mean_error, rowvar=False)\n",
    "diag_cov_matrix=np.diag(cov_matrix)\n",
    "# Create a diagonal matrix with the diagonal elements of the covariance matrix\n",
    "print(diag_cov_matrix)\n",
    "print(mean_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
